from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Callable
from enum import Enum
from datetime import datetime
import json
import sys
import os

# Add project root to python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from agents.pfc.ofc.ofc_main import OFC, ValuedStimulus, Valence, Priority
from langchain_groq import ChatGroq


class PredictionErrorType(Enum):
    """Type of reward prediction error."""
    POSITIVE = "positive"   # Better than expected â†’ reinforce
    NEGATIVE = "negative"   # Worse than expected â†’ adjust
    NEUTRAL = "neutral"     # As expected â†’ maintain


@dataclass
class OutcomeSignals:
    """
    Raw signals from the environment/body that compute actual utility.
    These are MEASURABLE, not LLM-generated.
    """
    # Task-related signals
    task_success_score: float = 0.0      # 0-1: Did the goal progress?
    goal_distance_delta: float = 0.0     # Negative = closer to goal, Positive = further
    
    # Stability signals
    replanning_count: int = 0            # How many times did we replan/loop?
    control_stability: float = 1.0       # 0-1: Was execution smooth?
    
    # Threat signals
    threat_level_before: float = 0.0     # Threat before action
    threat_level_after: float = 0.0      # Threat after action
    
    # Confidence signals
    confidence_before: float = 0.5       # Confidence before action
    confidence_after: float = 0.5        # Confidence after action
    
    # Physiological signals (from body/sensors)
    arousal_delta: float = 0.0           # Change in arousal/stress
    energy_cost: float = 0.0             # Resources expended (0-1)


@dataclass
class Outcome:
    """Actual outcome with computed utility (not LLM-generated)."""
    stimulus_source: str
    stimulus_content: str
    predicted_utility: float
    actual_utility: float              # COMPUTED from signals
    signals: OutcomeSignals            # Raw signals used for computation
    success: bool
    llm_reflection: Optional[str]      # LLM explanation (stored, NOT used for reward)
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class PredictionError:
    """Reward Prediction Error (RPE) signal."""
    stimulus_source: str
    stimulus_content: str
    predicted_utility: float
    actual_utility: float
    error_magnitude: float             # actual - predicted
    error_type: PredictionErrorType
    dopamine_signal: float             # Phasic dopamine burst/dip
    learning_signal: str               # What should be learned
    confidence_adjustment: float       # How much to trust this source
    signal_breakdown: Dict[str, float] # Component contributions


@dataclass
class CriticFeedback:
    """Feedback from Ventral Striatum to OFC and other regions."""
    prediction_errors: List[PredictionError]
    average_rpe: float
    bias_detected: Optional[str]
    recommended_modulation: Dict[str, float]
    source_reliability_updates: Dict[str, float]
    # New: signal-level insights
    component_biases: Dict[str, float]  # Which components are miscalibrated?


class VentralStriatum:
    """
    Ventral Striatum - Reward Prediction Error Critic
    
    KEY CHANGE: actual_utility is COMPUTED from measurable signals,
    NOT generated by LLM. LLM only provides reflection (stored separately).
    
    Utility Formula:
        actual_utility = (
            task_success_score          # did the goal progress?
          - replanning_penalty          # did we panic / loop?
          - threat_increase             # did danger rise?
          + confidence_stability        # did control feel stable?
          - energy_cost                 # resource expenditure
        )
    
    This creates a GROUNDED learning signal that can't be hallucinated.
    """
    
    # Component weights (tunable)
    WEIGHTS = {
        "task_success": 0.35,
        "goal_progress": 0.20,
        "replanning_penalty": 0.15,
        "threat_change": 0.15,
        "confidence_stability": 0.10,
        "energy_cost": 0.05,
    }
    
    def __init__(
        self,
        learning_rate: float = 0.1,
        memory_size: int = 100,
        surprise_threshold: float = 0.15,
        use_llm_reflection: bool = True,
    ) -> None:
        print(f"[VS] ðŸŽ¯ Initializing Ventral Striatum (Signal-Based Critic)...")
        
        self.learning_rate = learning_rate
        self.surprise_threshold = surprise_threshold
        self.memory_size = memory_size
        self.use_llm_reflection = use_llm_reflection
        
        # Experience memory
        self.outcome_history: List[Outcome] = []
        
        # Source reliability tracking
        self.source_reliability: Dict[str, float] = {}
        
        # OFC bias tracking
        self.ofc_bias_history: List[float] = []
        
        # Component-level bias tracking
        self.component_errors: Dict[str, List[float]] = {
            k: [] for k in self.WEIGHTS.keys()
        }
        
        # LLM for reflection only (not reward computation)
        if use_llm_reflection:
            self.llm = self._get_llm()
        else:
            self.llm = None
        
        print(f"[VS] ðŸŽ¯ Critic ready. Weights: {self.WEIGHTS}")
        print(f"[VS] ðŸŽ¯ LLM reflection: {'enabled' if use_llm_reflection else 'disabled'}")
    
    def _get_llm(self):
        api_key = "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"
        if not api_key:
            print("[VS] Warning: No API key, reflection disabled")
            return None
        try:
            return ChatGroq(
                model="openai/gpt-oss-120b",
                temperature=0.3,  # Slightly creative for reflection
                api_key=api_key
            )
        except Exception as e:
            print(f"[VS] Error init LLM: {e}")
            return None
    
    # =========================================================================
    # CORE: Compute actual utility from signals (NOT LLM)
    # =========================================================================
    
    def compute_actual_utility(self, signals: OutcomeSignals) -> Tuple[float, Dict[str, float]]:
        """
        Compute actual utility from measurable signals.
        
        This is the GROUNDED reward signal - no LLM hallucination possible.
        
        Returns:
            Tuple of (utility_score, component_breakdown)
        """
        components = {}
        
        # 1. Task success (positive contribution)
        components["task_success"] = signals.task_success_score * self.WEIGHTS["task_success"]
        
        # 2. Goal progress (negative delta = closer = good)
        goal_progress = -signals.goal_distance_delta  # Invert: closer = positive
        goal_progress = max(-1.0, min(1.0, goal_progress))  # Clamp
        components["goal_progress"] = goal_progress * self.WEIGHTS["goal_progress"]
        
        # 3. Replanning penalty (more replans = worse)
        replan_penalty = min(1.0, signals.replanning_count * 0.2)  # 5 replans = max penalty
        components["replanning_penalty"] = -replan_penalty * self.WEIGHTS["replanning_penalty"]
        
        # 4. Threat change (increase = bad, decrease = good)
        threat_delta = signals.threat_level_after - signals.threat_level_before
        components["threat_change"] = -threat_delta * self.WEIGHTS["threat_change"]
        
        # 5. Confidence stability (stable/increasing = good)
        conf_delta = signals.confidence_after - signals.confidence_before
        stability = signals.control_stability + (conf_delta * 0.5)
        stability = max(0.0, min(1.0, stability))
        components["confidence_stability"] = stability * self.WEIGHTS["confidence_stability"]
        
        # 6. Energy cost (higher cost = worse)
        components["energy_cost"] = -signals.energy_cost * self.WEIGHTS["energy_cost"]
        
        # Sum all components
        utility = sum(components.values())
        
        # Clamp to [-1, 1]
        utility = max(-1.0, min(1.0, utility))
        
        return utility, components
    
    # =========================================================================
    # LLM Reflection (parallel, stored separately, NOT used for reward)
    # =========================================================================
    
    def _get_llm_reflection(
        self,
        valued_stimulus: ValuedStimulus,
        signals: OutcomeSignals,
        computed_utility: float,
        context: Optional[str] = None,
    ) -> Optional[str]:
        """
        Get LLM reflection on the outcome.
        This is STORED but NOT used for reward computation.
        Useful for debugging and human interpretability.
        """
        if not self.llm:
            return None
        
        prompt = f"""Reflect on this outcome (for logging only, not reward):

STIMULUS: {valued_stimulus.source} - {valued_stimulus.content}
OFC PREDICTED: {valued_stimulus.utility_score:.2f}
ACTUAL (computed): {computed_utility:.2f}

SIGNALS:
- Task success: {signals.task_success_score:.2f}
- Goal distance delta: {signals.goal_distance_delta:.2f}
- Replanning count: {signals.replanning_count}
- Control stability: {signals.control_stability:.2f}
- Threat before/after: {signals.threat_level_before:.2f} â†’ {signals.threat_level_after:.2f}
- Confidence before/after: {signals.confidence_before:.2f} â†’ {signals.confidence_after:.2f}

CONTEXT: {context or 'None'}

Provide a brief 1-2 sentence reflection on why the prediction differed from reality."""

        try:
            response = self.llm.invoke([("user", prompt)])
            return response.content.strip()
        except Exception as e:
            return f"Reflection failed: {e}"
    
    # =========================================================================
    # Evaluate Outcome
    # =========================================================================
    
    def evaluate_outcome(
        self,
        valued_stimulus: ValuedStimulus,
        signals: OutcomeSignals,
        context: Optional[str] = None,
    ) -> Outcome:
        """
        Evaluate outcome using COMPUTED utility from signals.
        LLM reflection runs in parallel but is stored separately.
        """
        # CORE: Compute utility from signals (grounded)
        actual_utility, components = self.compute_actual_utility(signals)
        
        # Determine success
        success = actual_utility > 0 and signals.task_success_score > 0.5
        
        # LLM reflection (parallel, stored separately)
        reflection = None
        if self.use_llm_reflection:
            reflection = self._get_llm_reflection(
                valued_stimulus, signals, actual_utility, context
            )
        
        outcome = Outcome(
            stimulus_source=valued_stimulus.source,
            stimulus_content=valued_stimulus.content,
            predicted_utility=valued_stimulus.utility_score,
            actual_utility=actual_utility,
            signals=signals,
            success=success,
            llm_reflection=reflection,
        )
        
        # Store in history
        self._add_to_history(outcome)
        
        return outcome
    
    # =========================================================================
    # Compute Prediction Error
    # =========================================================================
    
    def compute_prediction_error(self, outcome: Outcome) -> PredictionError:
        """
        Compute Reward Prediction Error (RPE).
        Uses COMPUTED utility, not LLM-generated.
        """
        error = outcome.actual_utility - outcome.predicted_utility
        
        # Determine error type
        if error > self.surprise_threshold:
            error_type = PredictionErrorType.POSITIVE
        elif error < -self.surprise_threshold:
            error_type = PredictionErrorType.NEGATIVE
        else:
            error_type = PredictionErrorType.NEUTRAL
        
        # Dopamine signal
        if error_type == PredictionErrorType.POSITIVE:
            dopamine_signal = 1.0 + min(1.0, error * 2)
        elif error_type == PredictionErrorType.NEGATIVE:
            dopamine_signal = max(0.0, 0.5 + error)
        else:
            dopamine_signal = 1.0
        
        # Learning signal
        learning_signal = self._generate_learning_signal(outcome, error, error_type)
        
        # Confidence adjustment
        confidence_adj = self._compute_confidence_adjustment(
            outcome.stimulus_source, error
        )
        
        # Get component breakdown for analysis
        _, signal_breakdown = self.compute_actual_utility(outcome.signals)
        
        return PredictionError(
            stimulus_source=outcome.stimulus_source,
            stimulus_content=outcome.stimulus_content,
            predicted_utility=outcome.predicted_utility,
            actual_utility=outcome.actual_utility,
            error_magnitude=error,
            error_type=error_type,
            dopamine_signal=dopamine_signal,
            learning_signal=learning_signal,
            confidence_adjustment=confidence_adj,
            signal_breakdown=signal_breakdown,
        )
    
    def _generate_learning_signal(
        self,
        outcome: Outcome,
        error: float,
        error_type: PredictionErrorType,
    ) -> str:
        """Generate learning signal with component-level insight."""
        source = outcome.stimulus_source
        signals = outcome.signals
        
        # Identify which component contributed most to the error
        _, components = self.compute_actual_utility(signals)
        
        if error_type == PredictionErrorType.POSITIVE:
            # What made it better than expected?
            best_component = max(components, key=components.get)
            return (
                f"âœ¨ BETTER THAN EXPECTED (+{error:.2f}): "
                f"'{best_component}' contributed most. "
                f"Increase reward estimates for similar {source} inputs."
            )
        elif error_type == PredictionErrorType.NEGATIVE:
            # What made it worse?
            worst_component = min(components, key=components.get)
            return (
                f"âš ï¸ WORSE THAN EXPECTED ({error:.2f}): "
                f"'{worst_component}' was the issue. "
                f"Decrease utility estimates or increase threat sensitivity."
            )
        else:
            return f"âœ“ AS EXPECTED: Prediction accurate (error={error:.2f})."
    
    def _compute_confidence_adjustment(self, source: str, error: float) -> float:
        """Adjust confidence in a source based on prediction accuracy."""
        current = self.source_reliability.get(source, 0.5)
        accuracy = 1.0 - min(1.0, abs(error))
        adjustment = (accuracy - 0.5) * self.learning_rate
        new_reliability = max(0.1, min(1.0, current + adjustment))
        self.source_reliability[source] = new_reliability
        return new_reliability - current
    
    def _add_to_history(self, outcome: Outcome):
        """Add outcome to history with size limit."""
        self.outcome_history.append(outcome)
        if len(self.outcome_history) > self.memory_size:
            self.outcome_history.pop(0)
    
    # =========================================================================
    # Batch Critique
    # =========================================================================
    
    def critique_batch(
        self,
        outcomes: List[Tuple[ValuedStimulus, OutcomeSignals]],
        context: Optional[str] = None,
    ) -> CriticFeedback:
        """
        Critique a batch of stimulus-signal pairs.
        
        Args:
            outcomes: List of (ValuedStimulus, OutcomeSignals)
            context: Optional context
            
        Returns:
            CriticFeedback with learning signals
        """
        prediction_errors = []
        
        for valued_stim, signals in outcomes:
            outcome = self.evaluate_outcome(valued_stim, signals, context)
            rpe = self.compute_prediction_error(outcome)
            prediction_errors.append(rpe)
            
            # Track component-level errors for bias detection
            for component, value in rpe.signal_breakdown.items():
                self.component_errors[component].append(value)
        
        # Aggregate metrics
        avg_rpe = sum(pe.error_magnitude for pe in prediction_errors) / max(1, len(prediction_errors))
        self.ofc_bias_history.append(avg_rpe)
        
        # Detect bias
        bias = self._detect_bias()
        
        # Recommend modulation
        modulation = self._recommend_modulation(avg_rpe, prediction_errors)
        
        # Component-level bias analysis
        component_biases = self._analyze_component_biases()
        
        feedback = CriticFeedback(
            prediction_errors=prediction_errors,
            average_rpe=avg_rpe,
            bias_detected=bias,
            recommended_modulation=modulation,
            source_reliability_updates=dict(self.source_reliability),
            component_biases=component_biases,
        )
        
        self._print_critique(feedback)
        
        return feedback
    
    def _detect_bias(self) -> Optional[str]:
        """Detect systematic OFC prediction bias."""
        if len(self.ofc_bias_history) < 5:
            return None
        
        recent = self.ofc_bias_history[-10:]
        avg_bias = sum(recent) / len(recent)
        
        if avg_bias > 0.2:
            return "OFC UNDERESTIMATES value (pessimistic)"
        elif avg_bias < -0.2:
            return "OFC OVERESTIMATES value (optimistic)"
        return None
    
    def _analyze_component_biases(self) -> Dict[str, float]:
        """Analyze which signal components show consistent bias."""
        biases = {}
        for component, errors in self.component_errors.items():
            if len(errors) >= 5:
                recent = errors[-10:]
                avg = sum(recent) / len(recent)
                # Deviation from expected (0 = neutral)
                biases[component] = avg
        return biases
    
    def _recommend_modulation(
        self,
        avg_rpe: float,
        prediction_errors: List[PredictionError],
    ) -> Dict[str, float]:
        """Recommend neuromodulator adjustments."""
        recommendations = {}
        
        positive_count = sum(1 for pe in prediction_errors if pe.error_type == PredictionErrorType.POSITIVE)
        negative_count = sum(1 for pe in prediction_errors if pe.error_type == PredictionErrorType.NEGATIVE)
        total = max(1, len(prediction_errors))
        
        if positive_count / total > 0.6:
            recommendations["dopamine_adjust"] = +0.1
            recommendations["serotonin_adjust"] = -0.05
        elif negative_count / total > 0.6:
            recommendations["dopamine_adjust"] = -0.1
            recommendations["serotonin_adjust"] = +0.1
        else:
            recommendations["dopamine_adjust"] = 0.0
            recommendations["serotonin_adjust"] = 0.0
        
        return recommendations
    
    def _print_critique(self, feedback: CriticFeedback):
        """Pretty print critique output."""
        print("\n" + "=" * 100)
        print("VENTRAL STRIATUM CRITIQUE (Signal-Based)")
        print("=" * 100)
        print(f"{'SOURCE':<12} | {'PREDICTED':^10} | {'ACTUAL':^10} | {'RPE':^10} | {'DOPAMINE':^8} | KEY SIGNAL")
        print("-" * 100)
        
        for pe in feedback.prediction_errors:
            rpe_icon = "ðŸŸ¢" if pe.error_type == PredictionErrorType.POSITIVE else \
                      "ðŸ”´" if pe.error_type == PredictionErrorType.NEGATIVE else "âšª"
            
            # Find dominant signal component
            dominant = max(pe.signal_breakdown, key=lambda k: abs(pe.signal_breakdown[k]))
            
            print(
                f"{pe.stimulus_source:<12} | "
                f"{pe.predicted_utility:^10.2f} | "
                f"{pe.actual_utility:^10.2f} | "
                f"{rpe_icon} {pe.error_magnitude:+.2f}    | "
                f"{pe.dopamine_signal:^8.2f} | "
                f"{dominant}"
            )
        
        print("-" * 100)
        print(f"Average RPE: {feedback.average_rpe:+.3f}")
        
        if feedback.bias_detected:
            print(f"âš ï¸  BIAS: {feedback.bias_detected}")
        
        if feedback.component_biases:
            print(f"ðŸ“Š Component biases: {feedback.component_biases}")
        
        print("=" * 100)
    
    # =========================================================================
    # OFC Feedback Integration
    # =========================================================================
    
    def apply_feedback_to_ofc(self, ofc: OFC, feedback: CriticFeedback):
        """Apply critic feedback to OFC for online learning."""
        da_adj = feedback.recommended_modulation.get("dopamine_adjust", 0)
        st_adj = feedback.recommended_modulation.get("serotonin_adjust", 0)
        
        if da_adj != 0 or st_adj != 0:
            new_da = ofc.dopamine + da_adj
            new_st = ofc.serotonin + st_adj
            ofc.modulate(dopamine=new_da, serotonin=new_st)
            print(f"[VSâ†’OFC] Applied: DA={new_da:.2f}, 5-HT={new_st:.2f}")


# =============================================================================
# HELPER: Signal Collector (interface for other brain regions)
# =============================================================================

class SignalCollector:
    """
    Collects signals from various brain regions to feed Ventral Striatum.
    This is the interface between execution and critic.
    """
    
    def __init__(self):
        self.current_signals = OutcomeSignals()
        self._goal_state_before: Optional[float] = None
        self._threat_before: Optional[float] = None
        self._confidence_before: Optional[float] = None
    
    def begin_action(
        self,
        goal_distance: float = 0.0,
        threat_level: float = 0.0,
        confidence: float = 0.5,
    ):
        """Call before action execution to record baseline."""
        self._goal_state_before = goal_distance
        self._threat_before = threat_level
        self._confidence_before = confidence
        self.current_signals = OutcomeSignals(
            threat_level_before=threat_level,
            confidence_before=confidence,
        )
    
    def record_replanning(self):
        """Call each time replanning occurs."""
        self.current_signals.replanning_count += 1
    
    def end_action(
        self,
        task_success: float,
        goal_distance: float,
        threat_level: float,
        confidence: float,
        control_stability: float = 1.0,
        energy_cost: float = 0.0,
    ) -> OutcomeSignals:
        """Call after action to compute final signals."""
        self.current_signals.task_success_score = task_success
        self.current_signals.goal_distance_delta = goal_distance - (self._goal_state_before or goal_distance)
        self.current_signals.threat_level_after = threat_level
        self.current_signals.confidence_after = confidence
        self.current_signals.control_stability = control_stability
        self.current_signals.energy_cost = energy_cost
        
        return self.current_signals


# =============================================================================
# EXAMPLE USAGE
# =============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("VENTRAL STRIATUM - Signal-Based Critic Demo")
    print("=" * 60)
    
    # 1. Create OFC
    ofc = OFC(dopamine=1.0, serotonin=0.5)
    
    # 2. Simulate OFC predictions
    test_stimuli = [
        {
            "source": "vision",
            "content": "Smoke rising from kitchen",
            "attention": 0.9,
            "amygdala_label": "threat",
            "amygdala_salience": 0.8,
        },
        {
            "source": "hearing",
            "content": "Timer beeping",
            "attention": 0.6,
            "amygdala_label": "neutral",
            "amygdala_salience": 0.3,
        },
    ]
    
    print("\n--- 1. OFC PREDICTIONS ---")
    ofc_results = ofc.process_batch(test_stimuli, context="Cooking dinner")
    
    # 3. Create Ventral Striatum
    print("\n--- 2. ACTION EXECUTION & SIGNAL COLLECTION ---")
    vs = VentralStriatum(learning_rate=0.15, use_llm_reflection=True)
    collector = SignalCollector()
    
    # Simulate action for smoke stimulus
    collector.begin_action(goal_distance=0.5, threat_level=0.7, confidence=0.6)
    # ... action happens ...
    # It was just steam! Threat was overestimated
    signals_smoke = collector.end_action(
        task_success=0.8,
        goal_distance=0.3,      # Closer to goal
        threat_level=0.1,       # Threat decreased (was steam)
        confidence=0.8,
        control_stability=0.9,
        energy_cost=0.2,
    )
    
    # Simulate action for timer stimulus
    collector.begin_action(goal_distance=0.3, threat_level=0.1, confidence=0.8)
    signals_timer = collector.end_action(
        task_success=1.0,
        goal_distance=0.0,      # Goal achieved!
        threat_level=0.0,
        confidence=0.95,
        control_stability=1.0,
        energy_cost=0.1,
    )
    
    # 4. Critique
    print("\n--- 3. VENTRAL STRIATUM CRITIQUE ---")
    outcomes = [
        (ofc_results["ranked"][0], signals_smoke),
        (ofc_results["ranked"][1], signals_timer),
    ]
    
    feedback = vs.critique_batch(outcomes, context="Cooking dinner")
    
    # 5. Apply learning
    print("\n--- 4. APPLYING LEARNING TO OFC ---")
    vs.apply_feedback_to_ofc(ofc, feedback)
    
    # 6. Show reflections (stored but not used for reward)
    print("\n--- 5. LLM REFLECTIONS (Stored, Not Used for Reward) ---")
    for outcome in vs.outcome_history:
        print(f"\n[{outcome.stimulus_source}] {outcome.llm_reflection}")